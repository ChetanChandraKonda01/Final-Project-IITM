{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DaYzefTkm83",
        "outputId": "94c50c32-253d-402b-a7e6-59b80739acf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on MNIST\n",
            "Epoch [1/20], Loss: 0.1325\n",
            "Epoch [2/20], Loss: 0.0598\n",
            "Epoch [3/20], Loss: 0.0484\n",
            "Epoch [4/20], Loss: 0.0397\n",
            "Epoch [5/20], Loss: 0.0351\n",
            "Epoch [6/20], Loss: 0.0301\n",
            "Epoch [7/20], Loss: 0.0261\n",
            "Epoch [8/20], Loss: 0.0252\n",
            "Epoch [9/20], Loss: 0.0225\n",
            "Epoch [10/20], Loss: 0.0190\n",
            "Epoch [11/20], Loss: 0.0175\n",
            "Epoch [12/20], Loss: 0.0153\n",
            "Epoch [13/20], Loss: 0.0151\n",
            "Epoch [14/20], Loss: 0.0136\n",
            "Epoch [15/20], Loss: 0.0104\n",
            "Epoch [16/20], Loss: 0.0110\n",
            "Epoch [17/20], Loss: 0.0098\n",
            "Epoch [18/20], Loss: 0.0098\n",
            "Epoch [19/20], Loss: 0.0088\n",
            "Epoch [20/20], Loss: 0.0073\n",
            "Test Accuracy: 99.04%\n",
            "\n",
            "Training on FMNIST\n",
            "Epoch [1/20], Loss: 0.4241\n",
            "Epoch [2/20], Loss: 0.3022\n",
            "Epoch [3/20], Loss: 0.2610\n",
            "Epoch [4/20], Loss: 0.2397\n",
            "Epoch [5/20], Loss: 0.2201\n",
            "Epoch [6/20], Loss: 0.2041\n",
            "Epoch [7/20], Loss: 0.1874\n",
            "Epoch [8/20], Loss: 0.1773\n",
            "Epoch [9/20], Loss: 0.1559\n",
            "Epoch [10/20], Loss: 0.1417\n",
            "Epoch [11/20], Loss: 0.1275\n",
            "Epoch [12/20], Loss: 0.1173\n",
            "Epoch [13/20], Loss: 0.1048\n",
            "Epoch [14/20], Loss: 0.0976\n",
            "Epoch [15/20], Loss: 0.0815\n",
            "Epoch [16/20], Loss: 0.0755\n",
            "Epoch [17/20], Loss: 0.0655\n",
            "Epoch [18/20], Loss: 0.0607\n",
            "Epoch [19/20], Loss: 0.0559\n",
            "Epoch [20/20], Loss: 0.0524\n",
            "Test Accuracy: 91.60%\n",
            "\n",
            "Training on CIFAR-10\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/20], Loss: 1.3886\n",
            "Epoch [2/20], Loss: 1.0021\n",
            "Epoch [3/20], Loss: 0.8280\n",
            "Epoch [4/20], Loss: 0.7075\n",
            "Epoch [5/20], Loss: 0.6201\n",
            "Epoch [6/20], Loss: 0.5303\n",
            "Epoch [7/20], Loss: 0.4508\n",
            "Epoch [8/20], Loss: 0.3837\n",
            "Epoch [9/20], Loss: 0.3184\n",
            "Epoch [10/20], Loss: 0.2566\n",
            "Epoch [11/20], Loss: 0.2102\n",
            "Epoch [12/20], Loss: 0.1710\n",
            "Epoch [13/20], Loss: 0.1466\n",
            "Epoch [14/20], Loss: 0.1351\n",
            "Epoch [15/20], Loss: 0.1135\n",
            "Epoch [16/20], Loss: 0.1032\n",
            "Epoch [17/20], Loss: 0.0927\n",
            "Epoch [18/20], Loss: 0.0880\n",
            "Epoch [19/20], Loss: 0.0824\n",
            "Epoch [20/20], Loss: 0.0743\n",
            "Test Accuracy: 76.04%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Basic block for ResNet\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# ResNet18 model definition\n",
        "class ResNet18Modified(nn.Module):\n",
        "    def __init__(self, num_classes=10, in_channels=3):\n",
        "        super(ResNet18Modified, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, 2)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(in_channels, out_channels, stride, downsample))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BasicBlock(out_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Training and evaluation function\n",
        "def train_and_evaluate(model, train_loader, test_loader, num_epochs=20, learning_rate=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Loading and transforming datasets\n",
        "def load_data(dataset_name):\n",
        "    transform = transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])\n",
        "    if dataset_name == 'MNIST':\n",
        "        train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "        test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "        in_channels = 1\n",
        "    elif dataset_name == 'FMNIST':\n",
        "        train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "        test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
        "        in_channels = 1\n",
        "    elif dataset_name == 'CIFAR-10':\n",
        "        train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "        test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "        in_channels = 3\n",
        "    else:\n",
        "        raise ValueError(\"Dataset not supported\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "    return train_loader, test_loader, in_channels\n",
        "\n",
        "# Running training for each dataset\n",
        "for dataset_name in ['MNIST', 'FMNIST', 'CIFAR-10']:\n",
        "    print(f'\\nTraining on {dataset_name}')\n",
        "    train_loader, test_loader, in_channels = load_data(dataset_name)\n",
        "    model = ResNet18Modified(num_classes=10, in_channels=in_channels)\n",
        "    train_and_evaluate(model, train_loader, test_loader)\n"
      ]
    }
  ]
}